#coding=utf-8

import codecs
import json

from gensim import corpora
from nltk.tokenize.regexp import RegexpTokenizer
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from stop_words import get_stop_words
from projectSelect.smallcorpusTrain import smallCorpusTrain
import re

class Corpus:
    def __init__(self,dirname):
        self.dirname = dirname

    def __iter__(self):
        with codecs.open(self.dirname,'r','utf-8') as e:
            for line in e:
                data = json.loads(line)
                # resLs = [val for val in data if not re.match('\d*',val)]
                yield data

class Tags:
    def __init__(self,dirname):
        self.dirname = dirname

    def __iter__(self):
        with codecs.open(self.dirname,'r','utf-8') as e:
            lineNumber = -1
            for line in e:
                lineNumber+=1
                data = json.loads(line)
                # resLs = [val for val in data if not re.match('\d*',val)]
                yield {lineNumber:data}

    @classmethod
    def getDict(cls,dirname):
        retDict = {}
        try:
            with codecs.open(dirname,'r','utf-8') as e:
                lineNumber = -1
                for line in e:
                    lineNumber+=1
                    data = json.loads(line)
                    retDict[lineNumber] = [ls[0] for ls in data]
                    # resLs = [val for val in data if not re.match('\d*',val)]
        except Exception,e1:
            print dirname
        return retDict

    @classmethod
    def getDict1(cls,dirname):
        retDict = {}
        with codecs.open(dirname,'r','utf-8') as e:
            lineNumber = -1
            for line in e:
                lineNumber+=1
                data = json.loads(line)
                retDict[lineNumber] = data
                # resLs = [val for val in data if not re.match('\d*',val)]
        return retDict

class TokenWord:

    """å¾—åˆ°æ¯ä¸ªdocä¸­çš„word"""
    @staticmethod
    def getDoc_set(trainCorpusSentence,testCorpusSentence,trainWord,testWord):

        tokenizer = RegexpTokenizer(r'\w+')
        res = re.compile(r'(http://[\w\.\-/]+)')
        res1 = re.compile(r'((\w+\-)+\w+)')
        res2 = re.compile(r'(\w+\.\w+)|(\d+ns)')
        res3 = re.compile(r'#\d+')

        with open(trainWord,'w') as e:
            lineNumber = 0
            for doc in trainCorpusSentence:
                lineNumber += 1

                doc[0] = doc[0].replace("'s"," is")
                doc[0] = doc[0].replace("'re"," are")
                doc[0] = doc[0].replace("'m"," am")
                doc[0] = doc[0].replace("n't"," not")
                tmpWordLs = []

                for word in res.findall(doc[0]):
                    tmpWordLs.append(word)
                    doc[0] = doc[0].replace(word, '')
                for wordLs in res1.findall(doc[0]):
                    tmpWordLs.append(wordLs[0])
                    doc[0] = doc[0].replace(wordLs[0], '')
                for wordLs in res2.findall(doc[0]):
                    for word in wordLs:
                        if word != '':
                            tmpWordLs.append(word)
                            doc[0] = doc[0].replace(word, '')
                for word in res3.findall(doc[0]):
                    tmpWordLs.append(word)
                    doc[0] = doc[0].replace(word, '')

                raw = doc[0].lower()
                tokens = tokenizer.tokenize(raw)
                en_stop = get_stop_words("en")
                stopped_tokens = [i for i in tokens if i not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(i).encode('utf-8') for i in stopped_tokens]
                texts.extend(tmpWordLs)

                for word in texts:
                    if word.__len__() == 1:
                        print('train',word)
                        print(lineNumber)
                        break

                e.write(json.dumps(texts)+'\n')
        with open(testWord,'w') as e:
            for doc in testCorpusSentence:
                doc[0] = doc[0].replace("'s", " is")
                doc[0] = doc[0].replace("'re", " are")
                doc[0] = doc[0].replace("'m", " am")
                doc[0] = doc[0].replace("n't", " not")
                for word in res.findall(doc[0]):
                    tmpWordLs.append(word)
                    doc[0] = doc[0].replace(word, '')
                for wordLs in res1.findall(doc[0]):
                    tmpWordLs.append(wordLs[0])
                    doc[0] = doc[0].replace(wordLs[0], '')
                for wordLs in res2.findall(doc[0]):
                    for word in wordLs:
                        if word != '':
                            tmpWordLs.append(word)
                            doc[0] = doc[0].replace(word, '')
                for word in res3.findall(doc[0]):
                    tmpWordLs.append(word)
                    doc[0] = doc[0].replace(word, '')

                raw = doc[0].lower()
                tokens = tokenizer.tokenize(raw)
                en_stop = get_stop_words("en")
                stopped_tokens = [i for i in tokens if i not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(i).encode('utf-8') for i in stopped_tokens]
                texts.extend(tmpWordLs)

                for word in texts:
                    if word.__len__() == 1:
                        print('test',word)
                        print(doc)
                        break
                e.write(json.dumps(texts)+'\n')

    """ç”±äºgithubä¸Šå¥å­çš„ç‰¹æ®Šæ€§ï¼Œä»…ä»…ä½¿ç”¨åˆ‡å‰²çš„æ–¹æ³•ï¼Œå¾—åˆ°æ¯ä¸ªdocä¸­çš„word"""
    @staticmethod
    def getDoc_set_Simple(trainCorpusSentence,testCorpusSentence,trainWord,testWord):
        deleteWord = ['<','>','>=','<=','-','+','*']

        reg = re.compile('([\w\_]+\(.*?\))')  ###å–å‡ºå‡½æ•°
        reg1 = re.compile('[\r\n\"]')
        reg2 = re.compile('[\(\.\)]| - | \* |^- |^\* ')  ##å»æ‰ (, ), .
        reg3 = re.compile(' +')
        reg4 = re.compile('^ ')
        reg5 = re.compile(r'(\w+\.\w+)')
        reg6 = re.compile(r'>=|<=|;|<|>|=|`|\[|\]|/|\"')
        reg7 = re.compile(r'(https?://[\w\.\-/]+)')
        reg8 = re.compile(r'((\w+\-)+\w+)')
        ###å»é™¤é¦–éƒ¨ç©ºæ ¼
        with open(trainWord,'w') as e:
            lineNumber = 0
            for doc in trainCorpusSentence:
                lineNumber += 1

                doc[0] = re.sub(reg1,' ',doc[0])   ## å»æ‰\r \n
                doc[0] = doc[0].strip('.')
                wordLs = []

                for word in reg.findall(doc[0]):   ##  å–å‡ºå‡½æ•°
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word,'')
                for word in reg7.findall(doc[0]):  ##  å–å‡ºé“¾æ¥
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word,'')
                for ls in reg8.findall(doc[0]):  ## å»é™¤ www-wqww-ww
                    wordLs.append(ls[0])
                    doc[0] = doc[0].replace(ls[0], '')
                for word in reg5.findall(doc[0]):  ##  å–å‡ºå¸¦å°æ•°ç‚¹çš„æ•°å­—
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word,'')
                doc[0] = re.sub(reg2,' ', doc[0])
                doc[0] = re.sub(reg3,' ', doc[0])  ##å»é™¤ç©ºæ ¼
                doc[0] = re.sub(reg4,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼
                doc[0] = re.sub(reg6,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼

                doc[0] = doc[0].replace("'s"," is")
                doc[0] = doc[0].replace("'re"," are")
                doc[0] = doc[0].replace("'m"," am")
                doc[0] = doc[0].replace("n't"," not")
                doc[0] = doc[0].replace("(","")
                # doc[0] = doc[0].replace('[\(\)@\[\]\r\n]', '')

                tmpWordLs = doc[0].lower().strip().split(' ')
                en_stop = get_stop_words("en")
                stopped_tokens = [word.strip(",:") for word in tmpWordLs if word not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(word).encode('utf-8') for word in stopped_tokens]
                texts.extend(wordLs)

                for word in texts:
                    if word.__len__() == 1:
                        texts.remove(word)

                e.write(json.dumps(texts)+'\n')
        reg = re.compile('([\w\_]+\(.*?\))')  ###å–å‡ºå‡½æ•°
        reg1 = re.compile('[\r\n]')
        reg2 = re.compile('[\(\.\)]| - | \* |^- |^\* ')  ##å»æ‰ (, ), .
        reg3 = re.compile(' +')
        reg4 = re.compile('^ ')
        reg5 = re.compile(r'(\w+\.\w+)')
        reg6 = re.compile(r'>=|<=|;|<|>|=|`|\[|\]|/')
        reg7 = re.compile(r'(https?://[\w\.\-/]+)')
        ###å»é™¤é¦–éƒ¨ç©ºæ ¼
        with open(testWord,'w') as e:
            lineNumber = 0
            for doc in testCorpusSentence:
                lineNumber += 1

                doc[0] = re.sub(reg1,' ',doc[0])   ## å»æ‰\r \n
                wordLs = []
                for word in reg.findall(doc[0]):   ##  å–å‡ºå‡½æ•°
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word,'')
                for word in reg7.findall(doc[0]):  ##  å–å‡ºé“¾æ¥
                    wordLs.append(word.strip('.'))
                    doc[0] = doc[0].replace(word,'')

                for word in reg5.findall(doc[0]):  ##  å–å‡ºå¸¦å°æ•°ç‚¹çš„æ•°å­—
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word,'')
                doc[0] = re.sub(reg2,' ', doc[0])
                doc[0] = re.sub(reg3,' ', doc[0])  ##å»é™¤ç©ºæ ¼
                doc[0] = re.sub(reg4,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼
                doc[0] = re.sub(reg6,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼

                doc[0] = doc[0].replace("'s"," is")
                doc[0] = doc[0].replace("'re"," are")
                doc[0] = doc[0].replace("'m"," am")
                doc[0] = doc[0].replace("n't"," not")
                doc[0] = doc[0].replace("(","")
                # doc[0] = doc[0].replace('[\(\)@\[\]\r\n]', '')

                tmpWordLs = doc[0].lower().strip().split(' ')
                en_stop = get_stop_words("en")
                stopped_tokens = [word.strip(",:") for word in tmpWordLs if word not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(word).encode('utf-8') for word in stopped_tokens]
                texts.extend(wordLs)

                for word in texts:
                    if word.__len__() == 1:
                        texts.remove(word)

                e.write(json.dumps(texts)+'\n')
    """ç”±äºgithubä¸Šå¥å­çš„ç‰¹æ®Šæ€§ï¼Œä»…ä»…ä½¿ç”¨åˆ‡å‰²çš„æ–¹æ³•ï¼Œå¾—åˆ°æ¯ä¸ªdocä¸­çš„word"""
    @staticmethod
    def getDoc_set_Second(trainCorpusSentence,testCorpusSentence,trainWord,testWord,respOwnerAndNameLs):
        deleteWord = ['<','>','>=','<=','-','+','*']

        reg1 = re.compile('[\r\n\"]')
        reg2 = re.compile(r'(https://github.com/'+respOwnerAndNameLs[0]+'/'+respOwnerAndNameLs[1]+'/pull/(\d+))')
        reg3 = re.compile(' +')
        reg4 = re.compile('^ ')
        # reg5 = re.compile('(`\w+`)')
        reg6 = re.compile('(`[\w\-\.\*/:]+`)')
        reg7 = re.compile('(```[\s\S]+?```)')
        reg8 = re.compile('(\.\.\.[\s\S]+?\.\.\.)')
        reg9 = re.compile('(<img[\s\S]+?>)')
        reg10 = re.compile(r'(https?://[\w\.\-/]+)')

        res = re.compile("[\[*\]*\)\(>;_\|*]")   ##æ¶ˆé™¤wordä¸­çš„å­—ç¬¦
        res2 = re.compile('^\d[\d.]+\d$')
        res3 = re.compile('\*|`|-|##+|\[|\]|&+|\?|~|/|!=|\.||\\\\')


        ###å»é™¤é¦–éƒ¨ç©ºæ ¼
        with open(trainWord,'w') as e:
            lineNumber = 0
            for doc in trainCorpusSentence:
                lineNumber += 1

                doc[0] = re.sub(reg7,'',doc[0])   ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg6,'',doc[0])   ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg8,'',doc[0])   ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg9,'',doc[0])   ## å»æ‰å›¾ç‰‡

                doc[0] = re.sub(reg1,' ',doc[0])   ## å»æ‰\r \n
                doc[0] = doc[0].strip('.')
                wordLs = []

                for word in reg2.findall(doc[0]):  ##  å–å‡ºé“¾æ¥
                    wordLs.append("#"+str(word[1]))
                    doc[0] = doc[0].replace(word[0],'')
                doc[0] = re.sub(reg10, '', doc[0])  ## å»æ‰é“¾æ¥

                doc[0] = re.sub(reg3,' ', doc[0])  ##å»é™¤ç©ºæ ¼
                doc[0] = re.sub(reg4,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼

                doc[0] = doc[0].replace("'s"," is")
                doc[0] = doc[0].replace("'re"," are")
                doc[0] = doc[0].replace("'m"," am")
                doc[0] = doc[0].replace("n't"," not")
                doc[0] = doc[0].replace("(","")
                # doc[0] = doc[0].replace('[\(\)@\[\]\r\n]', '')

                tmpWordLs = doc[0].lower().strip().split(' ')
                en_stop = get_stop_words("en")
                stopped_tokens = [word.strip(",:.") for word in tmpWordLs if word not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(word).encode('utf-8') for word in stopped_tokens]
                texts.extend(wordLs)

                res_text = []
                for word in texts:

                    word = re.sub(res, '', word)
                    word = re.sub(res2, '', word)
                    word = re.sub(res3, '', word)
                    if word.__len__() > 1 and word not in en_stop:
                        res_text.append(word)

                e.write(json.dumps(res_text)+'\n')

        ###å»é™¤é¦–éƒ¨ç©ºæ ¼
        with open(testWord,'w') as e:
            lineNumber = 0
            for doc in testCorpusSentence:
                lineNumber += 1

                doc[0] = re.sub(reg7,'',doc[0])   ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg6,'',doc[0])   ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg8,'',doc[0])   ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg9,'',doc[0])   ## å»æ‰ä»£ç 

                doc[0] = re.sub(reg1,' ',doc[0])   ## å»æ‰\r \n
                doc[0] = doc[0].strip('.')
                wordLs = []

                for word in reg2.findall(doc[0]):  ##  å–å‡ºé“¾æ¥
                    wordLs.append("#"+str(word[1]))
                    doc[0] = doc[0].replace(word[0],'')

                doc[0] = re.sub(reg10, '', doc[0])  ## å»æ‰ä»£ç 
                doc[0] = re.sub(reg3,' ', doc[0])  ##å»é™¤ç©ºæ ¼
                doc[0] = re.sub(reg4,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼

                doc[0] = doc[0].replace("'s"," is")
                doc[0] = doc[0].replace("'re"," are")
                doc[0] = doc[0].replace("'m"," am")
                doc[0] = doc[0].replace("n't"," not")
                doc[0] = doc[0].replace("(","")
                # doc[0] = doc[0].replace('[\(\)@\[\]\r\n]', '')

                tmpWordLs = doc[0].lower().strip().split(' ')
                en_stop = get_stop_words("en")
                stopped_tokens = [word.strip(",:.") for word in tmpWordLs if word not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(word).encode('utf-8') for word in stopped_tokens]
                texts.extend(wordLs)

                res_text = []
                for word in texts:

                    word = re.sub(res, '', word)
                    word = re.sub(res2, '', word)
                    word = re.sub(res3, '', word)
                    if word.__len__() > 1 and word not in en_stop:
                        res_text.append(word)

                e.write(json.dumps(res_text)+'\n')

    """ç”±äºgithubä¸Šå¥å­çš„ç‰¹æ®Šæ€§ï¼Œä»…ä»…ä½¿ç”¨åˆ‡å‰²çš„æ–¹æ³•ï¼Œå¾—åˆ°æ¯ä¸ªdocä¸­çš„word"""
    @staticmethod
    def getDoc_set_First(trainCorpusSentence,testCorpusSentence,trainWord,testWord,respOwnerAndNameLs):
        deleteWord = ['<','>','>=','<=','-','+','*']

        reg1 = re.compile('[\r\n\"]')
        projectLink_reg = re.compile(r'(https://github.com/'+respOwnerAndNameLs[0]+'/'+respOwnerAndNameLs[1]+'/pull/(\d+))')
        reg3 = re.compile(' +')
        reg4 = re.compile('^ ')
        # reg5 = re.compile('(`\w+`)')
        code1_reg = re.compile('(`[\w\-\.\*/:]+`)')
        code2_reg = re.compile('(```[\s\S]+?```)')
        code3_reg = re.compile('(\.\.\.[\s\S]+?\.\.\.)')
        code4_reg = re.compile('(\*\*[\s\S]+?\*\*)')

        img_reg = re.compile('(<img[\s\S]+?>)')
        link_reg = re.compile(r'(https?://[\w\.\-/]+)')

        function_reg = re.compile('([\w\_]+\(.*?\))')  ###å–å‡ºå‡½æ•°

        res = re.compile("[\[*\]*\)\(>;_\|*]")   ##æ¶ˆé™¤wordä¸­çš„å­—ç¬¦
        res2 = re.compile('^\d[\d.]+\d$')
        res3 = re.compile('\*|`|-|##+|\[|\]|&+|\?|~|/|!=|\.|\\\\+|<+|=+')
        replaceStr = [u"â€œ",u'â€“',u"â€",u"â†’","(",u"Â ",u'ğŸµ',u'Â©',u'ğŸ˜',u'â€¦',u"â€˜",u"â€™",u"Â²",u"â‡’",u"à¸¿",u"â€”",u"Å‚"]

        ###å»é™¤é¦–éƒ¨ç©ºæ ¼
        with open(trainWord,'w') as e:
            lineNumber = 0
            for doc in trainCorpusSentence:
                lineNumber += 1
                doc[0] = re.sub(img_reg,'',doc[0])   ## å»æ‰å›¾ç‰‡

                doc[0] = re.sub(reg1,' ',doc[0])   ## å»æ‰\r \n
                doc[0] = doc[0].strip('.')
                wordLs = []

                for word in projectLink_reg.findall(doc[0]):  ##  å–å‡ºé“¾æ¥ä½œä¸ºä¸€ä¸ªè¯
                    wordLs.append("#" + str(word[1]))
                    doc[0] = doc[0].replace(word[0], '')
                for word in code1_reg.findall(doc[0]):
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word, '')
                for word in code2_reg.findall(doc[0]):  ###ä¸€å—åŒºåŸŸçš„å‡½æ•°
                    for subword in function_reg.findall(word):
                        wordLs.append(subword)
                        word = word.replace(subword, '')
                    wordLs.extend(word.strip().split(' '))
                    doc[0] = doc[0].replace(word, '')
                for word in code3_reg.findall(doc[0]):
                    for subword in function_reg.findall(word):
                        wordLs.append(subword)
                        word = word.replace(subword, '')
                    wordLs.extend(word.strip().split(' '))
                    doc[0] = doc[0].replace(word, '')
                for word in link_reg.findall(doc[0]):  ##  å–å‡ºé“¾æ¥
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word,'')

                doc[0] = re.sub(reg3,' ', doc[0])  ##å»é™¤ç©ºæ ¼
                doc[0] = re.sub(reg4,'', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼

                doc[0] = doc[0].replace(u"â€™s", " is")
                doc[0] = doc[0].replace("\u2019s", " is")
                doc[0] = doc[0].replace(u"â€™re", " are")
                doc[0] = doc[0].replace("\u2019re", " are")
                doc[0] = doc[0].replace(u"â€™m", " am")
                doc[0] = doc[0].replace("\u2019m", " am")
                doc[0] = doc[0].replace(u"nâ€™t", " not")
                doc[0] = doc[0].replace("n\u2019t", " not")
                for val in replaceStr:
                    doc[0] = doc[0].replace(val,"")
                # doc[0] = doc[0].replace('[\(\)@\[\]\r\n]', '')

                tmpWordLs = doc[0].lower().strip().split(' ')
                en_stop = get_stop_words("en")
                stopped_tokens = [word.strip(",:.") for word in tmpWordLs if word not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(word).encode('utf-8') for word in stopped_tokens]
                texts.extend(wordLs)

                res_text = []
                for word in texts:
                    word = word.replace(" ", "")
                    if isinstance(word,unicode):
                        for tmpStr in replaceStr:
                            word = word.replace(tmpStr, "")
                    word = re.sub(res, '', word)
                    word = re.sub(res2, '', word)
                    word = re.sub(res3, '', word)
                    if word.__len__() > 1 and word not in en_stop:
                        res_text.append(word)
                e.write(json.dumps(res_text)+'\n')

        ###å»é™¤é¦–éƒ¨ç©ºæ ¼
        with open(testWord, 'w') as e:
            lineNumber = 0
            for doc in testCorpusSentence:
                lineNumber += 1
                doc[0] = re.sub(img_reg, '', doc[0])  ## å»æ‰å›¾ç‰‡

                doc[0] = re.sub(reg1, ' ', doc[0])  ## å»æ‰\r \n
                doc[0] = doc[0].strip('.')
                wordLs = []

                for word in projectLink_reg.findall(doc[0]):  ##  å–å‡ºé“¾æ¥ä½œä¸ºä¸€ä¸ªè¯
                    wordLs.append("#" + str(word[1]))
                    doc[0] = doc[0].replace(word[0], '')
                for word in code1_reg.findall(doc[0]):
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word, '')
                for word in code2_reg.findall(doc[0]):  ###ä¸€å—åŒºåŸŸçš„å‡½æ•°
                    for subword in function_reg.findall(word):
                        wordLs.append(subword)
                        word = word.replace(subword, '')
                    wordLs.extend(word.strip().split(' '))
                    doc[0] = doc[0].replace(word, '')
                for word in code3_reg.findall(doc[0]):
                    for subword in function_reg.findall(word):
                        wordLs.append(subword)
                        word = word.replace(subword, '')
                    wordLs.extend(word.strip().split(' '))
                    doc[0] = doc[0].replace(word, '')
                for word in link_reg.findall(doc[0]):  ##  å–å‡ºé“¾æ¥
                    wordLs.append(word)
                    doc[0] = doc[0].replace(word, '')

                doc[0] = re.sub(reg3, ' ', doc[0])  ##å»é™¤ç©ºæ ¼
                doc[0] = re.sub(reg4, '', doc[0])  ##å»é™¤é¦–éƒ¨ç©ºæ ¼

                doc[0] = doc[0].replace(u"â€™s", " is")
                doc[0] = doc[0].replace("\u2019s", " is")
                doc[0] = doc[0].replace(u"â€™re", " are")
                doc[0] = doc[0].replace("\u2019re", " are")
                doc[0] = doc[0].replace(u"â€™m", " am")
                doc[0] = doc[0].replace("\u2019m", " am")
                doc[0] = doc[0].replace(u"nâ€™t", " not")
                doc[0] = doc[0].replace("n\u2019t", " not")
                for val in replaceStr:
                    doc[0] = doc[0].replace(val,"")
                # doc[0] = doc[0].replace('[\(\)@\[\]\r\n]', '')

                tmpWordLs = doc[0].lower().strip().split(' ')
                en_stop = get_stop_words("en")
                stopped_tokens = [word.strip(",:.") for word in tmpWordLs if word not in en_stop]
                p_stemmer = PorterStemmer()
                texts = [p_stemmer.stem(word).encode('utf-8') for word in stopped_tokens]
                texts.extend(wordLs)

                res_text = []
                for word in texts:
                    word = word.replace(" ","")
                    if isinstance(word,unicode):
                        for tmpStr in replaceStr:
                            word = word.replace(tmpStr, "")
                    word = re.sub(res, '', word)
                    word = re.sub(res2, '', word)
                    word = re.sub(res3, '', word)
                    if word.__len__() > 1 and word not in en_stop:
                        res_text.append(word)
                e.write(json.dumps(res_text) + '\n')

def mergeTrainAndTest(trainWord,testWord,outfilepath):
    with open(outfilepath,'w') as e:
        with open(trainWord,'r') as e1:
            for line in e1:
                e.write(line)
        with open(testWord,'r') as e2:
            for line in e2:
                e.write(line)

def getRespNameAndOwner(projectNameFile):
    with open(projectNameFile,'r') as e:
        for line in e:
            return json.loads(line)

if __name__ == '__main__':
    # basicFilepath = r'/media/mamile/DATA1/tagRecommendation_github/BP_rails/10ä¸ªé¡¹ç›®çš„CNNå®éªŒ(è¿‡æ»¤è®­ç»ƒé›†æ ‡ç­¾) /'
    #basicFilepath = r'/media/mamile/DATA1/tagRecommendation_github/BP_rails/10ä¸ªé¡¹ç›®çš„BPç¥ç»ç½‘ç»œå®éªŒ(5ä¸ªæœˆ)/'
    basicFilepath = r'/media/mamile/DATA1/tagRecommendation_github/BP_rails/10ä¸ªé¡¹ç›®çš„BPç¥ç»ç½‘ç»œå®éªŒ(è¿‡æ»¤è®­ç»ƒé›†æ ‡ç­¾)/'
    parameter = 2000
    projectDict = smallCorpusTrain.computePRofTestCorpusNumber(parameter)
    print(projectDict)
    for projectName in projectDict:
        if parameter == 4000 and projectName in ['bitcoin']: ###ç¬¬ä¸€ä¸ªè®­ç»ƒé›†çš„æ•°ç›®é€‰æ‹©4000æ—¶,bitcoinå°†ä¼šæ²¡æœ‰æµ‹è¯•é›†
            continue
        projectFile = projectName + 'é¡¹ç›®å®éªŒ/'
        projectNameFile = unicode(basicFilepath+projectFile+'é¡¹ç›®å.txt','utf-8')
        respOwnerAndNameLs = getRespNameAndOwner(projectNameFile)
        print(respOwnerAndNameLs)
        for i in xrange(1, projectDict[projectName] + 1):
            trainNumberFile = 'ç¬¬' + str(i) + 'æ¬¡è®­ç»ƒ/'
            trainBasicFile = basicFilepath + projectFile + trainNumberFile + 'trainCorpus/'
            testBasicFile = basicFilepath + projectFile + trainNumberFile + 'testCorpus/'
            newTrainCorpus = unicode(trainBasicFile +str(parameter)+'trainCorpus.txt', 'utf-8')
            # trainWord = unicode(trainBasicFile + 'second_trainCorpus_word.txt', 'utf-8')
            trainWord = unicode(trainBasicFile +str(parameter)+'_1_trainCorpus_word.txt', 'utf-8')
            print(trainWord)
            newTestCorpus = unicode(testBasicFile +str(parameter)+'testCorpus.txt', 'utf-8')
            # testWord = unicode(testBasicFile + 'second_testCorpus_word.txt', 'utf-8')
            testWord = unicode(testBasicFile +str(parameter)+'_1_testCorpus_word.txt', 'utf-8')

            trainCorpusSentence = Corpus(newTrainCorpus)
            testCorpusSentence = Corpus(newTestCorpus)

            # TokenWord.getDoc_set_Second(trainCorpusSentence,testCorpusSentence,trainWord,testWord,respOwnerAndNameLs)
            TokenWord.getDoc_set_First(trainCorpusSentence,testCorpusSentence,trainWord,testWord,respOwnerAndNameLs)